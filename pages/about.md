---
layout: page
title: About
permalink: /about/
weight: 3
---

# **About Me**

I am currently a post-doctoral research fellow in the Psychology department at UW-Madison working with <a href="https://www.niedenthalemotionslab.com/">Paula Niedenthal</a>. I received my Ph.D. from the Psychological and Brain Sciences department at Dartmouth College, where I worked with <a href="http://www.wheatlab.com/">Thalia Wheatley</a>. In the fall, I will be moving to Tennessee to start my own lab in the Psychology department at the University of Tennessee at Knoxville. I will be recruiting graduate students and undergraduate research assistants to work with me! 

Before beginning my graduate studies at Dartmouth, I received BAs in Music and Cognitive Science at the University of Virginia, where I worked with <a href="https://psychology.as.virginia.edu/people/profile/mk9y">Michael Kubovy</a> and <a href="https://sites.google.com/site/lauramariegetz/">Laura Getz</a>. After I graduated, I was a post-baccalaureate research assistant at the National Institutes of Health working with <a href="https://www.nimh.nih.gov/research/research-conducted-at-nimh/principal-investigators/alex-martin">Alex Martin</a>. 

# **About My Research**

My research program investigates how we, as individuals and social beings, coordinate our behavior and physiology to produce positive social interactions. To answer this question, my research links basic cognitive processes observed via rigorous, tightly controlled experimental paradigms with naturalistic studies of dyadic social interaction. My lab will use a range of computational methods, including natural language processing tools, cluster analyses and time series analyses. Current directions in my research include 1) studying individual differences in how people coordinate behaviors like facial expressions and attention-related pupil dilations, and 2) exploring how movements into and out of interpersonal synchrony over the course of a conversation may help to facilitate successful communication. my overarching goal is to study social coordination in its natural complexity to understand what connects us to each other when we interact.

# **Publications**

<i>*In prep*

Colon, Y., **Wohltjen, S.**, Tong, Y., & Niedenthal, P. (in prep). Feeling, movement, meaning: Exploring the conceptual space of dynamic facial expression. 

**Wohltjen, S.**, Goldman, M., & Wheatley, T. (in prep). Hanging on to almost every word: Pupillary asynchrony between storytellers and listeners predicts enhanced memory for surprising, exciting story moments. 

Zhu, Z., Huang, A., **Wohltjen, S.**, Niedenthal, P., & Li, Y. (in prep). An unsupervised and generative approach to categorizing facial expression. 

**Wohltjen, S.**, Colon, I., Zhu, Z., Huang, A., Miller, K., Li, Y., Mutlu, B., & Niedenthal, P. (in prep). A data-driven framework for discovering facial behaviors in the wild.

**Wohltjen, S.** & Wheatley, T. (in prep) Rapt with attention: Spontaneous eye blink rate decreases as conversation partners become more engaged.

<i>*Under review*

Niedenthal, P. & **Wohltjen, S.** (under review). Historical migration patterns and the emergence of culture. Featured review for special issue on historical psychology in Current Research in Ecological and Social Psychology.

<i>*Published*

**Wohltjen, S.**, Zhong, Y., Colon, I., Huang, A., Miller, K., Li, Y., Mutlu, B., & Niedenthal, P. (2025). Uniting theory and data: The promise and challenge of creating an honest model of facial expression. Cognition and Emotion, 1-15. <a href="https://doi.org/10.1080/02699931.2024.2446945">[link]</a>

**Wohltjen, S.**, & Wheatley, T. (2024). Interpersonal eye-tracking reveals the dynamics of interacting minds. Frontiers in Human Neuroscience –Interacting Minds and Brains, 18. <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2024.1356680/full">[link]</a>

Turkstra, L., Hosseini-Moghaddam, S., **Wohltjen, S.**, Nurre, S.V., Mutlu, B., & Duff, M. (2023). Facial affect recognition in context in adults with and without TBI. Frontiers in Psychology – Emotion Science, 14. <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1111686/full">[link]</a>

**Wohltjen, S.**, Toth, B., Boncz, A., & Wheatley, T. (2023). Synchrony to a beat predicts synchrony with other minds. Nature Scientific Reports, 13(3591). <a href="https://www.nature.com/articles/s41598-023-29776-6">[link]</a> <a href="https://github.com/sophiewohltjen/individual-attention">[data and code]</a>
Coverage: <a href="https://www.psychologytoday.com/gb/blog/the-sensory-revolution/202003/dont-got-rhythm-what-it-means-to-be-beat-deaf">Psychology Today</a>

**Wohltjen, S.** & Wheatley, T. (2021). Eye contact marks the rise and fall of shared attention in conversation. Proceedings of the National Academy of Sciences, 118(37), 1-8. <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2106645118">[link]</a> <a href="https://github.com/sophiewohltjen/eyeContact-in-conversation">[data and code]</a> 
Coverage: <a href="https://www.scientificamerican.com/article/making-eye-contact-signals-a-new-turn-in-a-conversation/">Scientific American</a>, <a href="https://www.forbes.com/sites/jarretjackson/2022/07/20/is-your-team-in-sync-if-not-it-may-be-time-for-a-game/?sh=1e0f21d974cb">Forbes</a>

Avery, J.A., Ingeholm, J.E., **Wohltjen, S.**, Collins, M., Riddell, C., Gotts, S.J., Kenworthy, L., Wallace, G., Simmons, K., & Martin, A. (2018). Neural correlates of taste reactivity in autism spectrum disorders. Neuroimage Clinical, 19, 38-46. <a href="https://doi.org/10.1016/j.biopsych.2018.02.397">[link]</a>

Mellem, M., **Wohltjen, S.**, Gotts, S., Ghuman, A., & Martin, A. (2017). Intrinsic frequency biases and profiles across human cortex. Journal of Neurophysiology. <a href="https://journals.physiology.org/doi/full/10.1152/jn.00061.2017">[link]</a>

Getz, L., **Wohltjen, S.**, & Kubovy, M. (2016). Competition between rhythmic and language organization: The importance of task demands. Quarterly Journal of Experimental Psychology, (69). <a href="https://doi.org/10.1080/17470218.2016.1173078">[link]</a>

